<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>AURA-X Î© â€“ Offline Emotional Continuity + Local LLM</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <script type="module">
    import * as webllm from "https://esm.run/@mlc-ai/web-llm";
    window.webllm = webllm; 
  </script>

  <style>
    /* Ø¢Ù¾ Ú©Ø§ Ù¾Ø±Ø§Ù†Ø§ CSS ÛŒÛØ§Úº Ø¨Ø±Ù‚Ø±Ø§Ø± Ø±ÛÛ’ Ú¯Ø§ */
    /* ... (Existing Styles from source 1 to 119) ... */
    
    /* Ù†ÛŒÙˆ Ø¨Ù¹Ù† Ú©Û’ Ù„ÛŒÛ’ ØªÚ¾ÙˆÚ‘Ø§ Ø§Ø¶Ø§ÙÛŒ Ø³Ù¹Ø§Ø¦Ù„ */
    .btn-offline { background: #10b981 !important; color: white !important; margin-top: 10px; }
    .offline-status { font-size: 0.7rem; color: #10b981; margin-top: 5px; }
  </style>
</head>
<body>
<div id="llmModal" class="modal-overlay">
  <div class="modal-card">
    <div class="modal-header">
      <div>
        <div class="modal-title">ğŸ¤– LLM link settings</div>
        <div class="modal-sub">Password 6789 Â· For private testing only.</div>
      </div>
      <button class="modal-close" data-close="llmModal">âœ•</button>
    </div>
    <div class="modal-body">
      <div id="llmLockStage">
        <p class="faith-note">Demo password: <strong>6789</strong>.</p>
        <input id="llmPasswordInput" type="password" class="modal-search" placeholder="Enter password (6789)">
        <button id="llmUnlockBtn" class="btn-mini" style="margin-top:8px;">Unlock</button>
      </div>
      <div id="llmConfigStage" style="display:none;">
        <div style="border-bottom: 1px solid #ddd; padding-bottom: 15px; margin-bottom: 15px;">
           <label style="font-size:.75rem; font-weight:bold; color:#0f172a;">LOCAL ENGINE (OFFLINE)</label>
           <p class="faith-note">Download 1GB+ model to browser for 100% offline chat.</p>
           <button id="loadOfflineModelBtn" class="btn-mini btn-offline">ğŸ“¥ Load TinyLlama (Offline)</button>
           <div id="offlineLoadingStatus" class="offline-status">Status: Not Loaded</div>
        </div>

        <label style="font-size:.75rem;color:#4b5563;display:block;margin-bottom:4px;">API base URL (Cloud)</label>
        <input id="llmBaseUrlInput" class="modal-search" />
        <label style="font-size:.75rem;color:#4b5563;display:block;margin-bottom:4px;">API key</label>
        <input id="llmKeyInput" type="password" class="modal-search" placeholder="sk-..." />
        <div style="display:flex;flex-wrap:wrap;gap:8px;margin-top:8px;">
          <button id="llmSaveBtn" class="btn-mini">Save Cloud Config</button>
          <button id="llmToggleBtn" class="btn-mini">Cloud LLM: OFF</button>
        </div>
      </div>
    </div>
  </div>
</div>

<script>
(function(){
  // ... (Existing Variables BM_BANDS etc.) ...

  class AuraXOmegaEngine {
    constructor(){
      // ... (Existing Constructor Logic source 155 to 176) ...
      this.localEngine = null;
      this.localModelId = "TinyLlama-1.1B-Chat-v1.0-q4f16_1-MLC";
      
      this.cacheDom();
      this.bindOfflineEvents(); // Ù†ÛŒØ§ ÙÙ†Ú©Ø´Ù†
      this.bindEvents();
      // ...
    }

    // Step 2: Offline Model Binding
    bindOfflineEvents() {
      const loadBtn = document.getElementById("loadOfflineModelBtn");
      const statusText = document.getElementById("offlineLoadingStatus");

      loadBtn.addEventListener("click", async () => {
        loadBtn.disabled = true;
        statusText.textContent = "Initializing WebGPU...";
        
        try {
          this.localEngine = await window.webllm.CreateMLCEngine(this.localModelId, {
            initProgressCallback: (report) => {
              statusText.textContent = `Downloading: ${Math.round(report.progress * 100)}%`;
            }
          });
          statusText.textContent = "Status: Offline Model Ready";
          loadBtn.style.display = "none";
        } catch (e) {
          statusText.textContent = "Error: " + e.message;
          loadBtn.disabled = false;
        }
      });
    }

    // Step 3: Updated callLLM logic (Hybrid Online/Offline)
    async callLLM(promptText) {
      // Priority A: Local Offline Model
      if (this.localEngine) {
        const reply = await this.localEngine.chat.completions.create({
          messages: [{ role: "user", content: promptText }]
        });
        return reply.choices[0].message.content;
      }

      // Priority B: Cloud API (Original Logic)
      if (this.llmConfig.apiKey && this.llmConfig.baseUrl) {
        // ... (Existing fetch code from source 536 to 539) ...
        const payload = {
          model: this.llmConfig.model || "gpt-4.1-mini",
          messages: [{role:"system", content:"AURA-X Î© Helper"}, {role:"user", content:promptText}],
          max_tokens: 256
        };
        const res = await fetch(this.llmConfig.baseUrl, {
          method: "POST",
          headers: { "Content-Type": "application/json", "Authorization": "Bearer " + this.llmConfig.apiKey },
          body: JSON.stringify(payload)
        });
        const data = await res.json();
        return data.choices[0].message.content.trim();
      }
      return null;
    }

    // ... (Ø¨Ø§Ù‚ÛŒ ØªÙ…Ø§Ù… ÙÙ†Ú©Ø´Ù†Ø² Ø¬ÛŒØ³Û’ analyzeSentimentØŒ generateReply ÙˆØºÛŒØ±Û Ù¾Ø±Ø§Ù†Û’ Ú©ÙˆÚˆ ÙˆØ§Ù„Û’ ÛÛŒ Ø±ÛÛŒÚº Ú¯Û’)
  }

  window.addEventListener("DOMContentLoaded", () => {
    window.auraX = new AuraXOmegaEngine();
  });
})();
</script>
</body>
</html>
