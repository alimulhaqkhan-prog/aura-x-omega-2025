<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>AURA-X Î© â€“ Offline Emotional Continuity Prototype</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <script type="module">
    import * as webllm from "https://esm.run/@mlc-ai/web-llm";
    window.webllm = webllm; 
  </script>

  <style>
    /* ... (Existing Styles from your code - Source 1 to 119) ... */
    .btn-offline { background: #10b981 !important; color: white !important; margin-top: 10px; }
    .offline-status { font-size: 0.7rem; color: #10b981; margin-top: 5px; }
  </style>
</head>
<body>
<div id="llmModal" class="modal-overlay">
  <div class="modal-card">
    <div class="modal-header">
      <div>
        <div class="modal-title">ðŸ¤– LLM Link & Offline Engine</div>
        [span_10](start_span)<div class="modal-sub">Password 6789[span_10](end_span)</div>
      </div>
      <button class="modal-close" data-close="llmModal">âœ•</button>
    </div>
    <div class="modal-body">
      <div id="llmLockStage">
        [span_11](start_span)<p class="faith-note">Demo password: <strong>6789</strong>[span_11](end_span).</p>
        <input id="llmPasswordInput" type="password" class="modal-search" placeholder="Enter password">
        <button id="llmUnlockBtn" class="btn-mini">Unlock</button>
      </div>
      <div id="llmConfigStage" style="display:none;">
        <div style="border-bottom: 1px solid #ddd; padding-bottom: 15px; margin-bottom: 15px;">
           <label style="font-weight:bold; color:#0f172a;">LOCAL ENGINE (OFFLINE)</label>
           <p class="faith-note">Load a 1GB model to browser memory for 100% offline intelligence.</p>
           <button id="loadOfflineModelBtn" class="btn-mini btn-offline">ðŸ“¥ Load TinyLlama (Offline)</button>
           <div id="offlineLoadingStatus" class="offline-status">Status: Not Loaded</div>
        </div>
        </div>
    </div>
  </div>
</div>

<script>
(function(){
  // ... (Existing Constants BM_BANDS etc.) ...

  class AuraXOmegaEngine {
    constructor(){
      // ... (Initializations from Source 155 to 174) ...
      this.localEngine = null;
      this.localModelId = "TinyLlama-1.1B-Chat-v1.0-q4f16_1-MLC"; // Efficient for mobile/web
      
      this.cacheDom();
      this.bindEvents();
      this.bindOfflineEvents(); // New binding
      // ...
    }

    bindOfflineEvents() {
      const loadBtn = document.getElementById("loadOfflineModelBtn");
      const statusText = document.getElementById("offlineLoadingStatus");

      loadBtn.onclick = async () => {
        loadBtn.disabled = true;
        statusText.textContent = "Initializing WebGPU...";
        try {
          this.localEngine = await window.webllm.CreateMLCEngine(this.localModelId, {
            initProgressCallback: (report) => {
              statusText.textContent = `Downloading: ${Math.round(report.progress * 100)}%`;
            }
          });
          statusText.textContent = "Status: Offline Model Ready";
          loadBtn.style.display = "none";
        } catch (e) {
          statusText.textContent = "Error: GPU not supported or download failed.";
          loadBtn.disabled = false;
        }
      };
    }

    // Updated generateReply to prioritize local memory then Local LLM
    async generateReply(text, sentiment, cloudAnswer, opts) {
      // 1. Check Local Intelligence (Seeds/Nodes)
      [span_12](start_span)[span_13](start_span)const intelMatch = this.matchIntelNode(text);[span_12](end_span)[span_13](end_span)
      [span_14](start_span)if (intelMatch) return intelMatch.node.answer;[span_14](end_span)

      [span_15](start_span)[span_16](start_span)const seed = this.matchSeedRule(text);[span_15](end_span)[span_16](end_span)
      [span_17](start_span)if (seed) return seed.reply;[span_17](end_span)

      // 2. Call Local LLM if loaded
      if (this.localEngine) {
        const bmContext = this.getBMContextSummary();
        const fullPrompt = `Context from Bold Memory: ${bmContext}\nUser says: ${text}\nAURA-X Î© Reply:`;
        const reply = await this.localEngine.chat.completions.create({
          messages: [{ role: "user", content: fullPrompt }]
        });
        return reply.choices[0].message.content;
      }

      // 3. Fallback to Cloud LLM or Default Reply
      [span_18](start_span)if (cloudAnswer) return cloudAnswer;[span_18](end_span)
      [span_19](start_span)return "I'm processing this through my continuity layers...[span_19](end_span)";
    }

    getBMContextSummary() {
      // Collects latest stories from BM to feed the LLM context
      [span_20](start_span)[span_21](start_span)return this.bmEntries.slice(-3).map(e => e.text).join(" ");[span_20](end_span)[span_21](end_span)
    }

    // Modified callLLM to handle cloud fallback (Source 534)
    async callLLM(promptText) {
      if (this.localEngine) return null; // Use local engine instead if active
      // ... (Original fetch logic from Source 536 to 539) ...
    }
  }

  window.addEventListener("DOMContentLoaded", () => {
    window.auraX = new AuraXOmegaEngine();
  });
})();
</script>
</body>
</html>
